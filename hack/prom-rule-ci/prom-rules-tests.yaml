---
rule_files:
  - /tmp/rules.verify

group_eval_order:
  - kubevirt.rules

tests:
  # Pod is using more CPU than expected
  - interval: 1m
    input_series:
      - series: 'node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate{namespace="ci",pod="virt-controller-8546c99968-x9jgg",node="node1"}'
        values: "2 2 2 2 2 2 2 2 2 2"
      - series: 'kube_pod_container_resource_requests{namespace="ci",container="virt-controller",resource="cpu",pod="virt-controller-8546c99968-x9jgg",node="node1"}'
        values: "0 0 0 0 0 0"

    alert_rule_test:
      - eval_time: 5m
        alertname: KubeVirtComponentExceedsRequestedCPU
        exp_alerts:
          - exp_annotations:
              description: "Container virt-controller in pod virt-controller-8546c99968-x9jgg cpu usage exceeds the CPU requested"
              summary: "The container is using more CPU than what is defined in the containers resource requests"
            exp_labels:
              severity: "warning"
              pod: "virt-controller-8546c99968-x9jgg"
              container: "virt-controller"
              namespace: ci
              node: node1
              resource: cpu

  # Pod is using more memory than expected
  - interval: 1m
    input_series:
      - series: 'container_memory_usage_bytes{namespace="ci",container="virt-controller",pod="virt-controller-8546c99968-x9jgg",node="node1"}'
        values: "157286400 157286400 157286400 157286400 157286400 157286400 157286400 157286400"
      - series: 'kube_pod_container_resource_requests{namespace="ci",container="virt-controller",resource="memory",pod="virt-controller-8546c99968-x9jgg",node="node1"}'
        values: "118325248 118325248 118325248 118325248 118325248 118325248 118325248 118325248"

    alert_rule_test:
      - eval_time: 5m
        alertname: KubeVirtComponentExceedsRequestedMemory
        exp_alerts:
          - exp_annotations:
              description: "Container virt-controller in pod virt-controller-8546c99968-x9jgg memory usage exceeds the memory requested"
              summary: "The container is using more memory than what is defined in the containers resource requests"
            exp_labels:
              severity: "warning"
              namespace: ci
              node: "node1"
              pod: "virt-controller-8546c99968-x9jgg"
              resource: "memory"
              container: virt-controller

  # All components are down
  - interval: 1m
    input_series:
      - series: 'up{namespace="ci", pod="virt-api-1"}'
        values: "0 0 0 0 0 0"
      - series: 'up{namespace="ci", pod="virt-controller-1"}'
        values: "0 0 0 0 0 0"
      - series: 'up{namespace="ci", pod="virt-operator-1"}'
        values: "0 0 0 0 0 0"

    alert_rule_test:
      - eval_time: 5m
        alertname: VirtAPIDown
        exp_alerts:
          - exp_annotations:
              summary: "All virt-api servers are down."
      - eval_time: 5m
        alertname: VirtControllerDown
        exp_alerts:
          - exp_annotations:
              summary: "No running virt-controller was detected for the last 5 min."
      - eval_time: 5m
        alertname: VirtOperatorDown
        exp_alerts:
          - exp_annotations:
              summary: "All virt-operator servers are down."

    # vmi running on a node without a virt-handler pod
  - interval: 1m
    input_series:
      - series: 'node_namespace_pod:kube_pod_info:{pod="virt-launcher-vmi", node="node01"}'
        values: "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1"
      - series: 'kube_pod_container_status_ready{pod="virt-handler-asdf", node="node01"}'
        values: "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"
      - series: 'node_namespace_pod:kube_pod_info:{pod="virt-handler-asdf", node="node01"}'
        values: "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"
      - series: 'kube_pod_container_status_ready{pod="virt-handler-asdfg", node="node02"}'
        values: "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1"
      - series: 'node_namespace_pod:kube_pod_info:{pod="virt-handler-asdfg", node="node02"}'
        values: "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1"

    alert_rule_test:
      - eval_time: 60m
        alertname: OrphanedVirtualMachineImages
        exp_alerts:
          - exp_annotations:
              summary: "No virt-handler pod detected on node node01 with running vmis for more than an hour"
            exp_labels:
              node: "node01"
              pod: "virt-handler-asdf"
              severity: "warning"

  # Some virt controllers are not ready
  - interval: 1m
    input_series:
      - series: 'kubevirt_virt_controller_ready{namespace="ci", pod="virt-controller-1"}'
        values: "1 1 1 1 1 1"
      - series: 'kubevirt_virt_controller_ready{namespace="ci", pod="virt-controller-2"}'
        values: "0 0 0 0 0 0"
      - series: 'up{namespace="ci", pod="virt-controller-1"}'
        values: "1 1 1 1 1 1"
      - series: 'up{namespace="ci", pod="virt-controller-2"}'
        values: "1 1 1 1 1 1"

    alert_rule_test:
      - eval_time: 5m
        alertname: LowReadyVirtControllersCount
        exp_alerts:
          - exp_annotations:
              summary: "Some virt controllers are running but not ready."

  # All virt controllers are not ready
  - interval: 1m
    input_series:
      - series: 'kubevirt_virt_controller_ready{namespace="ci", pod="virt-controller-1"}'
        values: "0 0 0 0 0 0"

    alert_rule_test:
      - eval_time: 5m
        alertname: NoReadyVirtController
        exp_alerts:
          - exp_annotations:
              summary: "No ready virt-controller was detected for the last 5 min."

  - interval: 1m
    input_series:
      - series: 'kubevirt_virt_controller_ready{namespace="ci", pod="virt-controller-1"}'
        values: "0 0 0 0 0 0"

    alert_rule_test:
      - eval_time: 5m
        alertname: NoReadyVirtController
        exp_alerts:
          - exp_annotations:
              summary: "No ready virt-controller was detected for the last 5 min."

  # High REST errors
  - interval: 1m
    input_series:
      - series: 'rest_client_requests_total{namespace="ci", pod="virt-controller-1", code="200"}'
        values: "2 2 2 2 2 2 2 2 2 2"
      - series: 'rest_client_requests_total{namespace="ci", pod="virt-controller-1", code="400"}'
        values: "10 10 10 10 10 10 10 10 10 10"
      - series: 'rest_client_requests_total{namespace="ci", pod="virt-operator-1", code="200"}'
        values: "2 2 2 2 2 2 2 2 2 2"
      - series: 'rest_client_requests_total{namespace="ci", pod="virt-operator-1", code="400"}'
        values: "10 10 10 10 10 10 10 10 10 20"
      - series: 'rest_client_requests_total{namespace="ci", pod="virt-handler-1", code="200"}'
        values: "2 2 2 2 2 2 2 2 2 2"
      - series: 'rest_client_requests_total{namespace="ci", pod="virt-handler-1", code="500"}'
        values: "10 10 10 10 10 10 10 10 10 20"

    alert_rule_test:
      - eval_time: 5m
        alertname: VirtControllerRESTErrorsHigh
        exp_alerts:
          - exp_annotations:
              summary: "More than 5% of the rest calls failed in virt-controller for the last hour"
            exp_labels:
              pod: "virt-controller-1"
      - eval_time: 5m
        alertname: VirtControllerRESTErrorsBurst
        exp_alerts:
          - exp_annotations:
              summary: "More than 80% of the rest calls failed in virt-controller for the last 5 minutes"
            exp_labels:
              pod: "virt-controller-1"
      - eval_time: 5m
        alertname: VirtOperatorRESTErrorsHigh
        exp_alerts:
          - exp_annotations:
              summary: "More than 5% of the rest calls failed in virt-operator for the last hour"
            exp_labels:
              pod: "virt-operator-1"
      - eval_time: 5m
        alertname: VirtOperatorRESTErrorsBurst
        exp_alerts:
          - exp_annotations:
              summary: "More than 80% of the rest calls failed in virt-operator for the last 5 minutes"
            exp_labels:
              pod: "virt-operator-1"
      - eval_time: 5m
        alertname: VirtHandlerRESTErrorsHigh
        exp_alerts:
          - exp_annotations:
              summary: "More than 5% of the rest calls failed in virt-handler for the last hour"
            exp_labels:
              pod: "virt-handler-1"
      - eval_time: 5m
        alertname: VirtHandlerRESTErrorsBurst
        exp_alerts:
          - exp_annotations:
              summary: "More than 80% of the rest calls failed in virt-handler for the last 5 minutes"
            exp_labels:
              pod: "virt-handler-1"

  # Some nodes without KVM resources
  - interval: 1m
    input_series:
      - series: 'kube_node_status_allocatable{resource="devices_kubevirt_io_kvm", node ="node1"}'
        values: "110 110 110 110 110 110"
      - series: 'kube_node_status_allocatable{resource="devices_kubevirt_io_kvm", node ="node2 "}'
        values: "0 0 0 0 0 0"

    alert_rule_test:
      - eval_time: 5m
        alertname: LowKVMNodesCount
        exp_alerts:
          - exp_annotations:
              description: "Low number of nodes with KVM resource available."
              summary: "At least two nodes with kvm resource required for VM life migration."
            exp_labels:
              severity: "warning"

  # All nodes without KVM resources
  - interval: 1m
    input_series:
      - series: 'kube_node_status_allocatable{resource="devices_kubevirt_io_kvm", node ="node1"}'
        values: "0 0 0 0 0 0"
      - series: 'kube_node_status_allocatable{resource="devices_kubevirt_io_kvm", node ="node2 "}'
        values: "0 0 0 0 0 0"

    alert_rule_test:
      - eval_time: 5m
        alertname: LowKVMNodesCount
        exp_alerts:
          - exp_annotations:
              description: "Low number of nodes with KVM resource available."
              summary: "At least two nodes with kvm resource required for VM life migration."
            exp_labels:
              severity: "warning"

  # Two nodes with KVM resources
  - interval: 1m
    input_series:
      - series: 'kube_node_status_allocatable{resource="devices_kubevirt_io_kvm", node ="node1"}'
        values: "110 110 110 110 110 110"
      - series: 'kube_node_status_allocatable{resource="devices_kubevirt_io_kvm", node ="node2 "}'
        values: "110 110 110 110 110 110"

    alert_rule_test:
      - eval_time: 5m
        alertname: LowKVMNodesCount
        exp_alerts: []

  # Memory utilization less than 20MB close to limit
  - interval: 1m
    input_series:
      - series: 'kube_pod_container_resource_limits_memory_bytes{pod="virt-launcher-testvm-123", container="compute"}'
        values: "67108864 67108864 67108864 67108864"
      - series: 'container_memory_working_set_bytes{pod="virt-launcher-testvm-123", container="compute"}'
        values: "47185920 48234496 48234496 49283072"

    alert_rule_test:
      - eval_time: 1m
        alertname: KubevirtVmHighMemoryUsage
        exp_alerts:
          - exp_annotations:
              description: "Container compute in pod virt-launcher-testvm-123 free memory is less than 20 MB and it is close to memory limit"
              summary: "VM is at risk of being terminated by the runtime."
            exp_labels:
              severity: "warning"
              pod: "virt-launcher-testvm-123"
              container: "compute"

  # Memory utilization more than 20MB close to limit
  - interval: 30s
    input_series:
      - series: 'kube_pod_container_resource_limits_memory_bytes{pod="virt-launcher-testvm-123", container="compute"}'
        values: "67108864 67108864 67108864 67108864"
      - series: 'container_memory_working_set_bytes{pod="virt-launcher-testvm-123", container="compute"}'
        values: "19922944 18874368 18874368 17825792"

    alert_rule_test:
      - eval_time: 1m
        alertname: KubevirtVmHighMemoryUsage
        exp_alerts: []

  # VM eviction strategy is set but vm is not migratable
  - interval: 1m
    input_series:
      - series: 'kubevirt_vmi_non_evictable{node="node1", namespace="ns-test", name="vm-evict-nonmigratable"}'
        values: "1 1 1 1 1 1 1 1"

    alert_rule_test:
      - eval_time: 1m
        alertname: VMCannotBeEvicted
        exp_alerts:
          - exp_annotations:
              description: "Eviction policy for vm-evict-nonmigratable (on node node1) is set to Live Migration but the VM is not migratable"
              summary: "The VM's eviction strategy is set to Live Migration but the VM is not migratable"
            exp_labels:
              severity: "warning"
              name: "vm-evict-nonmigratable"
              namespace: "ns-test"
              node: "node1"

  # VM eviction strategy is set and vm is migratable
  - interval: 1m
    input_series:
      - series: 'kubevirt_vmi_non_evictable{node="node1", namespace="ns-test", name="vm-evict-migratable"}'
        values: "0 0 0 0 0 0 0 0 "

    alert_rule_test:
      - eval_time: 1m
        alertname: VMCannotBeEvicted
        exp_alerts: []
