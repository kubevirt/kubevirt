---
rule_files:
  - /tmp/rules.verify

group_eval_order:
  - kubevirt.rules

tests:
  # Pod is using more CPU than expected
  - interval: 1m
    input_series:
      - series: 'node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate{namespace="ci",pod="virt-controller-8546c99968-x9jgg",node="node1"}'
        values: "2 2 2 2 2 2 2 2 2 2"
      - series: 'kube_pod_container_resource_requests{namespace="ci",container="virt-controller",resource="cpu",pod="virt-controller-8546c99968-x9jgg",node="node1"}'
        values: "0 0 0 0 0 0"

    alert_rule_test:
      - eval_time: 5m
        alertname: KubeVirtComponentExceedsRequestedCPU
        exp_alerts:
          - exp_annotations:
              description: "Container virt-controller in pod virt-controller-8546c99968-x9jgg cpu usage exceeds the CPU requested"
              summary: "The container is using more CPU than what is defined in the containers resource requests"
              runbook_url: "https://kubevirt.io/monitoring/runbooks/KubeVirtComponentExceedsRequestedCPU"
            exp_labels:
              severity: "warning"
              pod: "virt-controller-8546c99968-x9jgg"
              container: "virt-controller"
              namespace: ci
              node: node1
              resource: cpu

  # Pod is using more memory than expected
  - interval: 1m
    input_series:
      - series: 'container_memory_usage_bytes{namespace="ci",container="virt-controller",pod="virt-controller-8546c99968-x9jgg",node="node1"}'
        values: "157286400 157286400 157286400 157286400 157286400 157286400 157286400 157286400"
      - series: 'kube_pod_container_resource_requests{namespace="ci",container="virt-controller",resource="memory",pod="virt-controller-8546c99968-x9jgg",node="node1"}'
        values: "118325248 118325248 118325248 118325248 118325248 118325248 118325248 118325248"

    alert_rule_test:
      - eval_time: 5m
        alertname: KubeVirtComponentExceedsRequestedMemory
        exp_alerts:
          - exp_annotations:
              description: "Container virt-controller in pod virt-controller-8546c99968-x9jgg memory usage exceeds the memory requested"
              summary: "The container is using more memory than what is defined in the containers resource requests"
              runbook_url: "https://kubevirt.io/monitoring/runbooks/KubeVirtComponentExceedsRequestedMemory"
            exp_labels:
              severity: "warning"
              namespace: ci
              node: "node1"
              pod: "virt-controller-8546c99968-x9jgg"
              resource: "memory"
              container: virt-controller

  # Alerts to test whether our operators are up or not
  - interval: 1m
    input_series:
      - series: 'up{namespace="ci", pod="virt-api-1"}'
        values: "_ _ _ _ _ _ _ _ _ _ _ 0 0 0 0 0 0 1"
      - series: 'up{namespace="ci", pod="virt-controller-1"}'
        values: "_ _ _ _ _ _ _ _ _ _ _ 0 0 0 0 0 0 1"
      - series: 'up{namespace="ci", pod="virt-operator-1"}'
        values: "_ _ _ _ _ _ _ _ _ _ _ 0 0 0 0 0 0 1"

    alert_rule_test:
      # it must not trigger before 10m
      - eval_time: 8m
        alertname: VirtAPIDown
        exp_alerts: []
      - eval_time: 8m
        alertname: VirtControllerDown
        exp_alerts: [ ]
      - eval_time: 8m
        alertname: VirtOperatorDown
        exp_alerts: [ ]
      # it must trigger when there is no data
      - eval_time: 10m
        alertname: VirtAPIDown
        exp_alerts:
          - exp_annotations:
              summary: "All virt-api servers are down."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/VirtAPIDown"
            exp_labels:
              severity: "critical"
      - eval_time: 10m
        alertname: VirtControllerDown
        exp_alerts:
          - exp_annotations:
              summary: "No running virt-controller was detected for the last 10 min."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/VirtControllerDown"
            exp_labels:
              severity: "critical"
      - eval_time: 10m
        alertname: VirtOperatorDown
        exp_alerts:
          - exp_annotations:
              summary: "All virt-operator servers are down."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/VirtOperatorDown"
            exp_labels:
              severity: "critical"
      # it must trigger when operators are not healthy
      - eval_time: 16m
        alertname: VirtAPIDown
        exp_alerts:
          - exp_annotations:
              summary: "All virt-api servers are down."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/VirtAPIDown"
            exp_labels:
              severity: "critical"
      - eval_time: 16m
        alertname: VirtControllerDown
        exp_alerts:
          - exp_annotations:
              summary: "No running virt-controller was detected for the last 10 min."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/VirtControllerDown"
            exp_labels:
              severity: "critical"
      - eval_time: 16m
        alertname: VirtOperatorDown
        exp_alerts:
          - exp_annotations:
              summary: "All virt-operator servers are down."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/VirtOperatorDown"
            exp_labels:
              severity: "critical"
      # it must not trigger when operators are healthy
      - eval_time: 17m
        alertname: VirtAPIDown
        exp_alerts: []
      - eval_time: 17m
        alertname: VirtControllerDown
        exp_alerts: [ ]
      - eval_time: 17m
        alertname: VirtOperatorDown
        exp_alerts: [ ]


    # vmi running on a node without a virt-handler pod
  - interval: 1m
    input_series:
      - series: 'node_namespace_pod:kube_pod_info:{pod="virt-launcher-vmi", node="node01"}'
        values: "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1"
      - series: 'kube_pod_container_status_ready{pod="virt-handler-asdf", node="node01"}'
        values: "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"
      - series: 'node_namespace_pod:kube_pod_info:{pod="virt-handler-asdf", node="node01"}'
        values: "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"
      - series: 'kube_pod_container_status_ready{pod="virt-handler-asdfg", node="node02"}'
        values: "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1"
      - series: 'node_namespace_pod:kube_pod_info:{pod="virt-handler-asdfg", node="node02"}'
        values: "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1"

    alert_rule_test:
      - eval_time: 60m
        alertname: OrphanedVirtualMachineImages
        exp_alerts:
          - exp_annotations:
              summary: "No virt-handler pod detected on node node01 with running vmis for more than an hour"
              runbook_url: "https://kubevirt.io/monitoring/runbooks/OrphanedVirtualMachineImages"
            exp_labels:
              node: "node01"
              pod: "virt-handler-asdf"
              severity: "warning"

  # Some virt controllers are not ready
  - interval: 1m
    input_series:
      - series: 'kubevirt_virt_controller_ready{namespace="ci", pod="virt-controller-1"}'
        values: "1 1 1 1 1 1 1 1 1 1 1"
      - series: 'kubevirt_virt_controller_ready{namespace="ci", pod="virt-controller-2"}'
        values: "0 0 0 0 0 0 0 0 0 0 0"
      - series: 'up{namespace="ci", pod="virt-controller-1"}'
        values: "1 1 1 1 1 1 1 1 1 1 1"
      - series: 'up{namespace="ci", pod="virt-controller-2"}'
        values: "1 1 1 1 1 1 1 1 1 1 1"

    alert_rule_test:
      - eval_time: 10m
        alertname: LowReadyVirtControllersCount
        exp_alerts:
          - exp_annotations:
              summary: "Some virt controllers are running but not ready."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/LowReadyVirtControllersCount"
            exp_labels:
              severity: "warning"

  # All virt controllers are not ready
  - interval: 1m
    input_series:
      - series: 'kubevirt_virt_controller_ready{namespace="ci", pod="virt-controller-1"}'
        values: "0 0 0 0 0 0 0 0 0 0 0"

    alert_rule_test:
      # no alert before 10 minutes
      - eval_time: 9m
        alertname: NoReadyVirtController
        exp_alerts: [ ]
      - eval_time: 10m
        alertname: NoReadyVirtController
        exp_alerts:
          - exp_annotations:
              summary: "No ready virt-controller was detected for the last 10 min."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/NoReadyVirtController"
            exp_labels:
              severity: "critical"

  # All virt operators are not ready
  - interval: 1m
    input_series:
      - series: 'kubevirt_virt_operator_ready{namespace="ci", pod="virt-operator-1"}'
        values: "0 0 0 0 0 0 0 0 0 0 0"

    alert_rule_test:
      # no alert before 10 minutes
      - eval_time: 9m
        alertname: NoReadyVirtOperator
        exp_alerts: [ ]
      - eval_time: 10m
        alertname: NoReadyVirtOperator
        exp_alerts:
          - exp_annotations:
              summary: "No ready virt-operator was detected for the last 10 min."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/NoReadyVirtOperator"
            exp_labels:
              severity: "critical"

  # High REST errors
  - interval: 1m
    input_series:
      - series: 'rest_client_requests_total{namespace="ci", pod="virt-controller-1", code="200"}'
        values: "2 2 2 2 2 2 2 2 2 2"
      - series: 'rest_client_requests_total{namespace="ci", pod="virt-controller-1", code="400"}'
        values: "10 10 10 10 10 10 10 10 10 10"
      - series: 'rest_client_requests_total{namespace="ci", pod="virt-operator-1", code="200"}'
        values: "2 2 2 2 2 2 2 2 2 2"
      - series: 'rest_client_requests_total{namespace="ci", pod="virt-operator-1", code="400"}'
        values: "10 10 10 10 10 10 10 10 10 20"
      - series: 'rest_client_requests_total{namespace="ci", pod="virt-handler-1", code="200"}'
        values: "2 2 2 2 2 2 2 2 2 2"
      - series: 'rest_client_requests_total{namespace="ci", pod="virt-handler-1", code="500"}'
        values: "10 10 10 10 10 10 10 10 10 20"
      - series: 'rest_client_requests_total{namespace="ci", pod="virt-api-1", code="200"}'
        values: "2 2 2 2 2 2 2 2 2 2"
      - series: 'rest_client_requests_total{namespace="ci", pod="virt-api-1", code="500"}'
        values: "10 10 10 10 10 10 10 10 10 20"

    alert_rule_test:
      - eval_time: 5m
        alertname: VirtControllerRESTErrorsHigh
        exp_alerts:
          - exp_annotations:
              summary: "More than 5% of the rest calls failed in virt-controller for the last hour"
              runbook_url: "https://kubevirt.io/monitoring/runbooks/VirtControllerRESTErrorsHigh"
            exp_labels:
              pod: "virt-controller-1"
              severity: "warning"
      - eval_time: 5m
        alertname: VirtControllerRESTErrorsBurst
        exp_alerts:
          - exp_annotations:
              summary: "More than 80% of the rest calls failed in virt-controller for the last 5 minutes"
              runbook_url: "https://kubevirt.io/monitoring/runbooks/VirtControllerRESTErrorsBurst"
            exp_labels:
              pod: "virt-controller-1"
              severity: "critical"
      - eval_time: 5m
        alertname: VirtOperatorRESTErrorsHigh
        exp_alerts:
          - exp_annotations:
              summary: "More than 5% of the rest calls failed in virt-operator for the last hour"
              runbook_url: "https://kubevirt.io/monitoring/runbooks/VirtOperatorRESTErrorsHigh"
            exp_labels:
              pod: "virt-operator-1"
              severity: "warning"
      - eval_time: 5m
        alertname: VirtOperatorRESTErrorsBurst
        exp_alerts:
          - exp_annotations:
              summary: "More than 80% of the rest calls failed in virt-operator for the last 5 minutes"
              runbook_url: "https://kubevirt.io/monitoring/runbooks/VirtOperatorRESTErrorsBurst"
            exp_labels:
              pod: "virt-operator-1"
              severity: "critical"
      - eval_time: 5m
        alertname: VirtHandlerRESTErrorsHigh
        exp_alerts:
          - exp_annotations:
              summary: "More than 5% of the rest calls failed in virt-handler for the last hour"
              runbook_url: "https://kubevirt.io/monitoring/runbooks/VirtHandlerRESTErrorsHigh"
            exp_labels:
              pod: "virt-handler-1"
              severity: "warning"
      - eval_time: 5m
        alertname: VirtHandlerRESTErrorsBurst
        exp_alerts:
          - exp_annotations:
              summary: "More than 80% of the rest calls failed in virt-handler for the last 5 minutes"
              runbook_url: "https://kubevirt.io/monitoring/runbooks/VirtHandlerRESTErrorsBurst"
            exp_labels:
              pod: "virt-handler-1"
              severity: "critical"
      - eval_time: 5m
        alertname: VirtApiRESTErrorsHigh
        exp_alerts:
          - exp_annotations:
              summary: "More than 5% of the rest calls failed in virt-api for the last hour"
            exp_labels:
              pod: "virt-api-1"
              severity: "warning"
      - eval_time: 5m
        alertname: VirtApiRESTErrorsBurst
        exp_alerts:
          - exp_annotations:
              summary: "More than 80% of the rest calls failed in virt-api for the last 5 minutes"
            exp_labels:
              pod: "virt-api-1"
              severity: "critical"

  # Some nodes without KVM resources
  - interval: 1m
    input_series:
      - series: 'kube_node_status_allocatable{resource="devices_kubevirt_io_kvm", node ="node1"}'
        values: "110 110 110 110 110 110"
      - series: 'kube_node_status_allocatable{resource="devices_kubevirt_io_kvm", node ="node2 "}'
        values: "0 0 0 0 0 0"

    alert_rule_test:
      - eval_time: 5m
        alertname: LowKVMNodesCount
        exp_alerts:
          - exp_annotations:
              description: "Low number of nodes with KVM resource available."
              summary: "At least two nodes with kvm resource required for VM life migration."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/LowKVMNodesCount"
            exp_labels:
              severity: "warning"

  # All nodes without KVM resources
  - interval: 1m
    input_series:
      - series: 'kube_node_status_allocatable{resource="devices_kubevirt_io_kvm", node ="node1"}'
        values: "0 0 0 0 0 0"
      - series: 'kube_node_status_allocatable{resource="devices_kubevirt_io_kvm", node ="node2 "}'
        values: "0 0 0 0 0 0"

    alert_rule_test:
      - eval_time: 5m
        alertname: LowKVMNodesCount
        exp_alerts:
          - exp_annotations:
              description: "Low number of nodes with KVM resource available."
              summary: "At least two nodes with kvm resource required for VM life migration."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/LowKVMNodesCount"
            exp_labels:
              severity: "warning"

  # Two nodes with KVM resources
  - interval: 1m
    input_series:
      - series: 'kube_node_status_allocatable{resource="devices_kubevirt_io_kvm", node ="node1"}'
        values: "110 110 110 110 110 110"
      - series: 'kube_node_status_allocatable{resource="devices_kubevirt_io_kvm", node ="node2 "}'
        values: "110 110 110 110 110 110"

    alert_rule_test:
      - eval_time: 5m
        alertname: LowKVMNodesCount
        exp_alerts: []

  # Memory utilization less than 20MB close to limit
  - interval: 1m
    input_series:
      - series: 'kube_pod_container_resource_limits_memory_bytes{pod="virt-launcher-testvm-123", container="compute"}'
        values: "67108864 67108864 67108864 67108864"
      - series: 'container_memory_working_set_bytes{pod="virt-launcher-testvm-123", container="compute"}'
        values: "47185920 48234496 48234496 49283072"

    alert_rule_test:
      - eval_time: 1m
        alertname: KubevirtVmHighMemoryUsage
        exp_alerts:
          - exp_annotations:
              description: "Container compute in pod virt-launcher-testvm-123 free memory is less than 20 MB and it is close to memory limit"
              summary: "VM is at risk of being terminated by the runtime."
              runbook_url: "https://kubevirt.io/monitoring/runbooks/KubevirtVmHighMemoryUsage"
            exp_labels:
              severity: "warning"
              pod: "virt-launcher-testvm-123"
              container: "compute"

  # Memory utilization more than 20MB close to limit
  - interval: 30s
    input_series:
      - series: 'kube_pod_container_resource_limits_memory_bytes{pod="virt-launcher-testvm-123", container="compute"}'
        values: "67108864 67108864 67108864 67108864"
      - series: 'container_memory_working_set_bytes{pod="virt-launcher-testvm-123", container="compute"}'
        values: "19922944 18874368 18874368 17825792"

    alert_rule_test:
      - eval_time: 1m
        alertname: KubevirtVmHighMemoryUsage
        exp_alerts: []

  # VM eviction strategy is set but vm is not migratable
  - interval: 1m
    input_series:
      - series: 'kubevirt_vmi_non_evictable{node="node1", namespace="ns-test", name="vm-evict-nonmigratable"}'
        values: "1 1 1 1 1 1 1 1"

    alert_rule_test:
      - eval_time: 1m
        alertname: VMCannotBeEvicted
        exp_alerts:
          - exp_annotations:
              description: "Eviction policy for vm-evict-nonmigratable (on node node1) is set to Live Migration but the VM is not migratable"
              summary: "The VM's eviction strategy is set to Live Migration but the VM is not migratable"
              runbook_url: "https://kubevirt.io/monitoring/runbooks/VMCannotBeEvicted"
            exp_labels:
              severity: "warning"
              name: "vm-evict-nonmigratable"
              namespace: "ns-test"
              node: "node1"

  # VM eviction strategy is set and vm is migratable
  - interval: 1m
    input_series:
      - series: 'kubevirt_vmi_non_evictable{node="node1", namespace="ns-test", name="vm-evict-migratable"}'
        values: "0 0 0 0 0 0 0 0 "

    alert_rule_test:
      - eval_time: 1m
        alertname: VMCannotBeEvicted
        exp_alerts: []
